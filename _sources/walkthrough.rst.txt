Data Reduction Walkthrough
==========================

Input Data
----------
Since version *0.4.0* the pipeline assumes the input data are similar to how they will come out of the DHS. There will
be one FITS file for each SysOutput, and each file will have a single HDU with both the data and a
SPEC-0122-compliant header. It is possible to put different types of exposures (darks, gains, etc.) into separate
folders, but this is not necessary; the pipeline will segregate data based on the ``DKIST004`` header keyword.

Input Data
----------
For this walkthrough we will use a set of fake data generated by `vbi_fake_data` which generates a full suite of input
data. To do so we'll need two sets of data; some images of the Sun from the DST and a set of simulated GOS target images.
You can download the `DST data here <https://drive.google.com/file/d/109KcM3BMI-lXHHlgcqBt6Cz6TA-5t1Rp/view?usp=sharing>`_
and the `GOS images here <https://drive.google.com/file/d/1ItlFUZ_JS58N9jecXdVG6dZXT6ibS2ON/view?usp=sharing>`_. Once
you've extracted those tarballs use the following command to generate a fake data set:

.. code-block:: console

  $ vbi_fake_data DST_data GOS_grids raw
  ...
  $ ls -1 raw
  VBI-RED_20180511T142400.000000_0.fits
  VBI-RED_20180511T142400.033333_1.fits
  VBI-RED_20180511T142400.066667_2.fits
  VBI-RED_20180511T142400.100000_3.fits
  VBI-RED_20180511T142405.000000_0.fits
  VBI-RED_20180511T142405.033333_1.fits
  VBI-RED_20180511T142405.066667_2.fits
  VBI-RED_20180511T142405.100000_3.fits
  VBI-RED_20180511T142410.000000_0.fits

There are a lot of options to `vbi_fake_data`. Check 'em out with the ``-h`` option.

Setup
-----
To keep things organized lets use a different directory to hold our pipeline outputs and create a default config file
there

.. code-block:: console

  $ mkdir rdx
  $ cd rdx
  $ vbi_pipeline -d config.ini

The only parameters we *need* to change in ``config.ini`` are the input directories and maybe the ``data_set_ID``.
Because all of our calibration data are in the same folder we'll just replace all instances of "REPLACE_ME" with
"../input_data". Let's also take advantage of a special feature and set ``data_set_ID = all``, which will find each
target in the input directory and reduce them separately. In this case ``output`` now defines a prefix for the final
files.

.. code-block:: ini

 [Main]
 raw_sci_dir = ../raw
 data_set_id = all
 output = Sci
 ; the following 3 options point to where these objects will be saved. they don't need to exist
 dark_cal = DarkCal.fits
 gain_cal = GainCal.fits
 target_cal = TargetCal.fits
 ; if flush is true memory usage will be very conservative at the expense of more disk usage
 flush = False

 [DarkCalibration]
 raw_dark_dir = ../raw

 [GainCalibration]
 raw_gain_dir = ../raw

 [TargetCalibration]
 raw_grid_dir = ../raw

Run
---
This is the easy part, just call the pipeline

.. code-block:: console

  $ vbi_pipeline config.ini

Depending on which camera you're using this might take between 3 and 8 minutes. The VBI Pipeline produces a lot of
status messages and it is usually a good idea to save these somewhere so you can see exactly what was done to the data
at a later time (and find any error messages). To do this I usually run the pipeline as

.. code-block:: console

  $PYTHONUNBUFFERED=yes vbi_pipeline config.ini 2>&1 | tee spool.txt

The `PYTHONUNBUFFERED=yes` is there because on most systems simply piping the python output will cause your terminal
(stdout) to update much less frequently than usual. This is due to how python decides to flush its buffer to stdout.

Output Data
-----------
If your config file looks like the one above then your reduction directory will now look like this

.. code-block:: console

  $ ls -1
  config.ini
  DarkCal.fits
  GainCal.fits
  grid_proc.fits
  Sci_GOStarget.fits
  Sci_sunspot1.fits
  TargetCal.fits
  TargetCal_houghres.pkl

All of the ``*Cal.fits`` files are the intermediate data products used by the pipeline whose names are specified in the
config file. ``grid_proc.fits`` and ``TargetCal_houghres.pkl`` are special intermediates to the Target Calibration
module and exist to speed up future reduction runs (assuming the parameters stay the same).

The processed data live in ``Sci_*.fits``. These files each contain a single Primary HDU with no data and a set of Image
HDUs, one for each SysOutput of the input data. This format is suitable for further scientific analysis but if
you just want to see what the full image looks like you can stitch together the entire VBI field with ``vbi_stitch``:

.. code-block:: console

  $ vbi_stitch Sci_GOStarget.fits Sci_GOStarget_stitched.fits

The stitching process first averages the data to produce a single frame for each camera position and then places all
of the camera positions in their correct location in a single frame. This process necessarily interpolates the data
and flux conservation is not guaranteed so the result might not meet your requirements for further analysis.

Once all of the intermediate data products are generated you can go back and just change ``data_set_ID`` and ``output``
to push different science objects through the same pipeline. All subsequent runs will be much faster.